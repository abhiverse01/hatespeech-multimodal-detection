{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Modal Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries (fixed order and added missing imports)\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Verify early CUDA availability and set device\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Path Configuration\n",
    "PROJECT_ROOT = os.path.dirname(os.getcwd())  # Assuming notebook is in notebooks/\n",
    "DATASET_PATH = os.path.join(PROJECT_ROOT, \"notebooks/dataset\")\n",
    "EMBEDDINGS_PATH = os.path.join(PROJECT_ROOT, \"notebooks/embeddings/embeddings_0001_50samples.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multimodal Dataset Class\n",
    "import ast\n",
    "\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "        self.valid_indices = self._validate_samples()\n",
    "\n",
    "    def _validate_samples(self):\n",
    "        valid = []\n",
    "        for idx in range(len(self.data)):\n",
    "            try:\n",
    "                row = self.data.iloc[idx]\n",
    "                img_path = os.path.join(DATASET_PATH, \"transformed_images\", f\"{idx}.pt\")\n",
    "                if os.path.exists(img_path):\n",
    "                    valid.append(idx)\n",
    "            except:\n",
    "                continue\n",
    "        return valid\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = self.valid_indices[index]\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        input_ids_path = os.path.join(DATASET_PATH, row['input_ids'])\n",
    "        attention_mask_path = os.path.join(DATASET_PATH, row['attention_mask'])\n",
    "        image_path = os.path.join(DATASET_PATH, \"transformed_images\", f\"{idx}.pt\")\n",
    "\n",
    "        input_ids = torch.load(input_ids_path).squeeze(0)  # [1, 128] → [128]\n",
    "        attention_mask = torch.load(attention_mask_path).squeeze(0) # [1, 128] → [128]\n",
    "        # Load image tensor directly from the file\n",
    "        image = torch.load(image_path)\n",
    "        labels = torch.tensor(row['labels'], dtype=torch.float)\n",
    "        # Return labels to tensor\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'image': image,\n",
    "            'labels': labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Collate Function: Handles padding and batching\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return {\n",
    "            'input_ids': torch.zeros((1, 128), dtype=torch.long),\n",
    "            'attention_mask': torch.zeros((1, 128), dtype=torch.long),\n",
    "            'image': torch.zeros((1, 3, 224, 224)),\n",
    "            'labels': torch.zeros((1, 6))\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'image': torch.stack([item['image'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Data Preparation\n",
    "df = pd.read_csv(os.path.join(DATASET_PATH, \"dataset_transformed.csv\"))\n",
    "df['labels'] = df['labels'].apply(eval)\n",
    "\n",
    "# Split data\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 samples validation:\n",
      "Sample 0: True\n",
      "Sample 1: True\n",
      "Sample 2: True\n",
      "Sample 3: True\n",
      "Sample 4: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Dataset Verification\n",
    "print(\"First 5 samples validation:\")\n",
    "for idx in range(5):\n",
    "    sample_path = os.path.join(DATASET_PATH, \"transformed_images\", f\"{idx}.pt\")\n",
    "    print(f\"Sample {idx}: {os.path.exists(sample_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Dataloader Initialization\n",
    "train_dataset = MultiModalDataset(train_df)\n",
    "val_dataset = MultiModalDataset(val_df)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HateSpeechClassifier Model\n",
    "\n",
    "class HateSpeechClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text_encoder = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.img_encoder = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "        self.img_encoder = nn.Sequential(*list(self.img_encoder.children())[:-1])\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768 + 2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask']\n",
    "        ).last_hidden_state[:, 0, :]  # [CLS] token\n",
    "\n",
    "        img_features = self.img_encoder(batch['image']).squeeze(-1).squeeze(-1)  # [B, 2048, 1, 1] → [B, 2048]\n",
    "        combined = torch.cat((text_features, img_features), dim=1)  # [B, 768+2048]\n",
    "\n",
    "        return self.classifier(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nmodel = HateSpeechClassifier().to(device)\\noptimizer = optim.AdamW(model.parameters(), lr=2e-5)\\ncriterion = nn.BCEWithLogitsLoss()\\n'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 9: Training Setup\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 3\n",
    "\n",
    "model = HateSpeechClassifier().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# If class is more imbalanced, we will use pos_weighted loss\n",
    "\"\"\"\n",
    "pos_weight = torch.tensor([1.0, 2.0, 3.0, 3.0, 4.0, 3.0]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\"\"\"\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Train Loss: 0.6956\n",
      "Val Loss: 0.6639\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-Hate       0.89      0.89      0.89         9\n",
      "      Racist       0.00      0.00      0.00         4\n",
      "      Sexist       1.00      0.50      0.67         2\n",
      "  Homophobic       0.20      1.00      0.33         1\n",
      "    Religion       0.00      0.00      0.00         0\n",
      "  Other Hate       0.20      1.00      0.33         1\n",
      "\n",
      "   micro avg       0.48      0.65      0.55        17\n",
      "   macro avg       0.38      0.56      0.37        17\n",
      "weighted avg       0.61      0.65      0.59        17\n",
      " samples avg       0.51      0.72      0.56        17\n",
      "\n",
      "✅ Best model saved.\n",
      "Epoch 2/2\n",
      "Train Loss: 0.6480\n",
      "Val Loss: 0.6434\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-Hate       0.90      1.00      0.95         9\n",
      "      Racist       0.00      0.00      0.00         4\n",
      "      Sexist       0.00      0.00      0.00         2\n",
      "  Homophobic       0.33      1.00      0.50         1\n",
      "    Religion       0.00      0.00      0.00         0\n",
      "  Other Hate       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.59      0.59      0.59        17\n",
      "   macro avg       0.21      0.33      0.24        17\n",
      "weighted avg       0.50      0.59      0.53        17\n",
      " samples avg       0.72      0.67      0.65        17\n",
      "\n",
      "✅ Best model saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nfor epoch in range(5):\\n    model.train()\\n    total_loss = 0.0\\n    \\n    for batch in train_loader:\\n        optimizer.zero_grad()\\n        inputs = {k: v.to(device) for k, v in batch.items()}\\n        outputs = model(inputs)\\n        loss = criterion(outputs, inputs[\\'labels\\'])\\n        loss.backward()\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\\n        optimizer.step()\\n        total_loss += loss.item()\\n\\n    # Validation\\n    model.eval()\\n    val_loss = 0.0\\n    with torch.no_grad():\\n        for batch in val_loader:\\n            inputs = {k: v.to(device) for k, v in batch.items()}\\n            outputs = model(inputs)\\n            val_loss += criterion(outputs, inputs[\\'labels\\']).item()\\n    \\n\\n    print(f\"Epoch {epoch+1}/5\")\\n    print(f\"Train Loss: {total_loss/len(train_loader):.4f}\")\\n    print(f\"Val Loss: {val_loss/len(val_loader):.4f}\\n\")\\n\\n\\n'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 10: Training Loop\n",
    "\n",
    "# Hate Speech Labels\n",
    "LABELS = [\n",
    "    \"Non-Hate\",\n",
    "    \"Racist\",\n",
    "    \"Sexist\",\n",
    "    \"Homophobic\",\n",
    "    \"Religion\",     \n",
    "    \"Other Hate\"   \n",
    "]\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs['labels'])\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs['labels'])\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy() > 0.5\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(inputs['labels'].cpu().numpy())\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Metrics\n",
    "    preds_all = np.vstack(all_preds)\n",
    "    labels_all = np.vstack(all_labels)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(labels_all, preds_all, target_names=LABELS, zero_division=0))\n",
    "\n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        wait = 0\n",
    "        torch.save(model.state_dict(), os.path.join(PROJECT_ROOT, \"app/best_model_weights.pth\"))\n",
    "        print(\"✅ Best model saved.\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= PATIENCE:\n",
    "            print(\"⛔ Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
