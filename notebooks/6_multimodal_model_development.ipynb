{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Modal Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries (fixed order and added missing imports)\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Verify early CUDA availability and set device\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Path Configuration\n",
    "PROJECT_ROOT = os.path.dirname(os.getcwd())  # Assuming notebook is in notebooks/\n",
    "DATASET_PATH = os.path.join(PROJECT_ROOT, \"notebooks/dataset\")\n",
    "EMBEDDINGS_PATH = os.path.join(PROJECT_ROOT, \"notebooks/embeddings/embeddings_0000_50samples.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multimodal Dataset Class\n",
    "import ast\n",
    "\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "        self.valid_indices = self._validate_samples()\n",
    "\n",
    "    def _validate_samples(self):\n",
    "        valid = []\n",
    "        for idx in range(len(self.data)):\n",
    "            try:\n",
    "                row = self.data.iloc[idx]\n",
    "                img_path = os.path.join(DATASET_PATH, \"transformed_images\", f\"{idx}.pt\")\n",
    "                if os.path.exists(img_path):\n",
    "                    valid.append(idx)\n",
    "            except:\n",
    "                continue\n",
    "        return valid\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = self.valid_indices[index]\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        input_ids_path = os.path.join(DATASET_PATH, row['input_ids'])\n",
    "        attention_mask_path = os.path.join(DATASET_PATH, row['attention_mask'])\n",
    "        image_path = os.path.join(DATASET_PATH, \"transformed_images\", f\"{idx}.pt\")\n",
    "\n",
    "        input_ids = torch.load(input_ids_path).squeeze(0)  # [1, 128] → [128]\n",
    "        attention_mask = torch.load(attention_mask_path).squeeze(0) # [1, 128] → [128]\n",
    "        # Load image tensor directly from the file\n",
    "        image = torch.load(image_path)\n",
    "        labels = torch.tensor(row['labels'], dtype=torch.float)\n",
    "        # Return labels to tensor\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'image': image,\n",
    "            'labels': labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Collate Function: Handles padding and batching\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return {\n",
    "            'input_ids': torch.zeros((1, 128), dtype=torch.long),\n",
    "            'attention_mask': torch.zeros((1, 128), dtype=torch.long),\n",
    "            'image': torch.zeros((1, 3, 224, 224)),\n",
    "            'labels': torch.zeros((1, 3))\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'image': torch.stack([item['image'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Data Preparation\n",
    "df = pd.read_csv(os.path.join(DATASET_PATH, \"dataset_transformed.csv\"))\n",
    "df['labels'] = df['labels'].apply(eval)\n",
    "\n",
    "# Split data\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 samples validation:\n",
      "Sample 0: True\n",
      "Sample 1: True\n",
      "Sample 2: True\n",
      "Sample 3: True\n",
      "Sample 4: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Dataset Verification\n",
    "print(\"First 5 samples validation:\")\n",
    "for idx in range(5):\n",
    "    sample_path = os.path.join(DATASET_PATH, \"transformed_images\", f\"{idx}.pt\")\n",
    "    print(f\"Sample {idx}: {os.path.exists(sample_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Dataloader Initialization\n",
    "train_dataset = MultiModalDataset(train_df)\n",
    "val_dataset = MultiModalDataset(val_df)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HateSpeechClassifier Model\n",
    "\n",
    "class HateSpeechClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text_encoder = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.img_encoder = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "        self.img_encoder = nn.Sequential(*list(self.img_encoder.children())[:-1])\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768 + 2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask']\n",
    "        ).last_hidden_state[:, 0, :]  # [CLS] token\n",
    "\n",
    "        img_features = self.img_encoder(batch['image']).squeeze(-1).squeeze(-1)  # [B, 2048, 1, 1] → [B, 2048]\n",
    "        combined = torch.cat((text_features, img_features), dim=1)  # [B, 768+2048]\n",
    "\n",
    "        return self.classifier(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Training Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HateSpeechClassifier().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.6592\n",
      "Val Loss: 0.6376\n",
      "\n",
      "Epoch 2/5\n",
      "Train Loss: 0.5801\n",
      "Val Loss: 0.6258\n",
      "\n",
      "Epoch 3/5\n",
      "Train Loss: 0.4930\n",
      "Val Loss: 0.6153\n",
      "\n",
      "Epoch 4/5\n",
      "Train Loss: 0.3358\n",
      "Val Loss: 0.6143\n",
      "\n",
      "Epoch 5/5\n",
      "Train Loss: 0.2644\n",
      "Val Loss: 0.6248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Training Loop\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs['labels'])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, inputs['labels']).item()\n",
    "    \n",
    "\n",
    "    print(f\"Epoch {epoch+1}/5\")\n",
    "    print(f\"Train Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    print(f\"Val Loss: {val_loss/len(val_loader):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Model Saving\n",
    "torch.save(model.state_dict(), os.path.join(PROJECT_ROOT, \"app/model_weights.pth\"))\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
